{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from nebu.processors.decorate import processor\n",
    "from nebu import Message\n",
    "from nebu.processors.models import (\n",
    "    V1Scale,\n",
    "    V1ScaleDown,\n",
    "    V1ScaleUp,\n",
    "    V1ScaleZero,\n",
    ")\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Decorator Init] @processor decorating function 'train_unsloth_sft'\n",
      "[DEBUG Decorator] Determining execution environment...\n",
      "[DEBUG Helper] Checking if running in Jupyter...\n",
      "[DEBUG Helper] is_jupyter_notebook: IPython class name: <class 'ipykernel.zmqshell.ZMQInteractiveShell'>\n",
      "[DEBUG Helper] is_jupyter_notebook: Jupyter detected (ZMQInteractiveShell).\n",
      "[DEBUG Decorator] Jupyter environment detected.\n",
      "[DEBUG Helper] Attempting to get notebook execution history...\n",
      "[DEBUG Helper] get_notebook_executed_code: Retrieved 2 history entries.\n",
      "[DEBUG Helper] get_notebook_executed_code: Total history source length: 7889\n",
      "[DEBUG Decorator] Retrieved notebook history (length: 7889).\n",
      "[DEBUG Decorator] No manually included objects specified.\n",
      "[DEBUG Decorator] Validating signature and type hints for train_unsloth_sft...\n",
      "[DEBUG Decorator] Raw type hints: {'message': <class 'nebu.processors.models.Message[TrainingRequest]'>, 'return': <class '__main__.TrainingResponse'>}\n",
      "[DEBUG Decorator] Parameter 'message' type hint: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator] Return type hint: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] Determining input type structure for param type hint: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator] get_origin result: None, get_args result: ()\n",
      "[DEBUG Decorator] get_origin failed. Attempting regex fallback on type string: '<class 'nebu.processors.models.Message[TrainingRequest]'>'\n",
      "[DEBUG Decorator] Regex matched generic Message pattern!\n",
      "[DEBUG Decorator] Captured content type name via regex: 'TrainingRequest'\n",
      "[DEBUG Decorator] Successfully resolved content type name 'TrainingRequest' to type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Final Input Type Determination: is_stream_message=True, content_type=<class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Validating parameter and return types are BaseModel subclasses...\n",
      "[DEBUG Decorator] check_basemodel: Checking Parameter 'message' - Type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Parameter 'message': <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Parameter 'message' effective type (TrainingRequest) is a BaseModel subclass.\n",
      "[DEBUG Decorator] check_basemodel: Checking Return value - Type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Return value: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Return value effective type (TrainingResponse) is a BaseModel subclass.\n",
      "[DEBUG Decorator] Type validation complete.\n",
      "[DEBUG Decorator] Getting source code for function 'train_unsloth_sft'...\n",
      "[DEBUG Decorator] Attempting notebook history extraction for function 'train_unsloth_sft'...\n",
      "[DEBUG Helper] Extracting 'train_unsloth_sft' (FunctionDef) from history string (len: 7889)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'train_unsloth_sft' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'train_unsloth_sft'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'train_unsloth_sft' from cell #1.\n",
      "[DEBUG Decorator] Found function 'train_unsloth_sft' source in notebook history.\n",
      "[DEBUG Decorator] Final function source obtained for 'train_unsloth_sft' (len: 6298). Source starts:\n",
      "-------def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\n",
      "    import time\n",
      "    from unsloth import FastVisionModel, is_bf16_supported\n",
      "    from unsloth.trainer import UnslothVisionDataCollator\n",
      "    from trl import SFTTrainer, SFTConf...\n",
      "-------\n",
      "[DEBUG Decorator] Getting model sources...\n",
      "[DEBUG Decorator] Getting base Message source...\n",
      "[DEBUG get_type_source] Getting source for type: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG get_model_source] Getting source for: Message\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: Message\n",
      "[DEBUG Helper] Extracting 'Message' (ClassDef) from history string (len: 7889)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Definition 'Message' of type ClassDef not found in history search.\n",
      "[DEBUG get_model_source] Notebook history extraction failed for: Message. Proceeding to dill.\n",
      "[DEBUG get_model_source] Attempting dill fallback for: Message\n",
      "[DEBUG get_model_source] Using dill source for: Message\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG Decorator] Input is StreamMessage. Content type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Getting source for content_type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG get_type_source] Getting source for type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG get_model_source] Getting source for: TrainingRequest\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: TrainingRequest\n",
      "[DEBUG Helper] Extracting 'TrainingRequest' (ClassDef) from history string (len: 7889)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'TrainingRequest' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'TrainingRequest'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'TrainingRequest' from cell #1.\n",
      "[DEBUG get_model_source] Using notebook history source for: TrainingRequest\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Getting source for return_type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG get_type_source] Getting source for type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG get_model_source] Getting source for: TrainingResponse\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: TrainingResponse\n",
      "[DEBUG Helper] Extracting 'TrainingResponse' (ClassDef) from history string (len: 7889)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'TrainingResponse' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'TrainingResponse'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'TrainingResponse' from cell #1.\n",
      "[DEBUG get_model_source] Using notebook history source for: TrainingResponse\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] Source Result - Content Type: Found\n",
      "[DEBUG Decorator] Source Result - Input Model (non-stream): Not Found or N/A\n",
      "[DEBUG Decorator] Source Result - Output Model: Found\n",
      "[DEBUG Decorator] Source Result - Base StreamMessage: Found\n",
      "[DEBUG Decorator] Populating environment variables...\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'INPUT_MODEL'\n",
      "[DEBUG Decorator] add_source_to_env: No source for 'INPUT_MODEL', skipping.\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'OUTPUT_MODEL'\n",
      "[DEBUG Decorator] Added env var OUTPUT_MODEL_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'CONTENT_TYPE'\n",
      "[DEBUG Decorator] Added env var CONTENT_TYPE_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'STREAM_MESSAGE'\n",
      "[DEBUG Decorator] Added env var STREAM_MESSAGE_SOURCE (string)\n",
      "[DEBUG Decorator] Adding type info env vars...\n",
      "[DEBUG Decorator] Finished populating environment variables.\n",
      "[DEBUG Decorator] Preparing final Processor object...\n",
      "[DEBUG Decorator] Adding setup script to command.\n",
      "[DEBUG Decorator] Final container command:\n",
      "-------\n",
      "python -m pip install dill pydantic redis nebu\n",
      "\n",
      "\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
      "pip uninstall -y xformers\n",
      "pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
      "pip install unsloth trl\n",
      "\n",
      "\n",
      "python -u -m nebu.processors.consumer\n",
      "-------\n",
      "[DEBUG Decorator] Final Container Request Env Vars (Summary):\n",
      "[DEBUG Decorator]  FUNCTION_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  FUNCTION_NAME: train_unsloth_sft\n",
      "[DEBUG Decorator]  OUTPUT_MODEL_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  CONTENT_TYPE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  STREAM_MESSAGE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  PARAM_TYPE_STR: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator]  RETURN_TYPE_STR: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator]  IS_STREAM_MESSAGE: True\n",
      "[DEBUG Decorator]  CONTENT_TYPE_NAME: TrainingRequest\n",
      "[DEBUG Decorator]  MODULE_NAME: __main__\n",
      "Using namespace: pbarker\n",
      "Existing processors: processors=[]\n",
      "Processor: None\n",
      "Creating processor\n",
      "Request:\n",
      "{'kind': 'Processor', 'metadata': {'name': 'train_unsloth_sft', 'namespace': 'pbarker'}, 'container': {'kind': 'Container', 'platform': 'runpod', 'metadata': {'name': 'train_unsloth_sft'}, 'image': 'pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel', 'env': [{'key': 'FUNCTION_SOURCE', 'value': 'def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\\n    import time\\n    from unsloth import FastVisionModel, is_bf16_supported\\n    from unsloth.trainer import UnslothVisionDataCollator\\n    from trl import SFTTrainer, SFTConfig\\n    from nebu import (\\n        Bucket,\\n        ContainerConfig,\\n        Cache,\\n        Adapter,\\n        find_latest_checkpoint,\\n        is_allowed,\\n        oai_to_unsloth,\\n    )\\n    import requests\\n    import json\\n\\n    print(\"message\", message)\\n    training_request: TrainingRequest = message.content\\n    if not training_request:\\n        raise ValueError(\"No training request provided\")\\n\\n    container_config = ContainerConfig.from_env()\\n    print(\"container_config\", container_config)\\n\\n    cache = Cache()\\n    bucket = Bucket()\\n\\n    print(\"loading model...\")\\n    adapter_uri = f\"{container_config.namespace_volume_uri}/adapters/{training_request.adapter_name}\"\\n    time_start_load = time.time()\\n    model = None\\n\\n    cache_key = f\"adapters:{training_request.adapter_name}\"\\n    print(\"checking cache for adapter\", cache_key)\\n    val_raw = cache.get(cache_key)\\n\\n    is_continue = False\\n    if val_raw:\\n        adapter = Adapter.model_validate_json(val_raw)\\n        print(\"Found adapter: \", adapter)\\n\\n        if not is_allowed(adapter.owner, message.user_id, message.orgs):\\n            raise ValueError(\"You are not allowed to train this existing adapter\")\\n\\n        time_start = time.time()\\n        bucket.sync(adapter.uri, \"/latest\")\\n        print(f\"Synced in {time.time() - time_start} seconds\")\\n\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            \"/latest\",\\n            load_in_4bit=False,\\n            use_gradient_checkpointing=\"unsloth\",\\n        )\\n        is_continue = True\\n    if not model:\\n        print(\"Loading model from scratch\")\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            training_request.model,\\n            load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.\\n            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\\n        )\\n\\n        print(\"getting peft model...\")\\n        model = FastVisionModel.get_peft_model(\\n            model,\\n            finetune_vision_layers=True,  # False if not finetuning vision layers\\n            finetune_language_layers=True,  # False if not finetuning language layers\\n            finetune_attention_modules=True,  # False if not finetuning attention layers\\n            finetune_mlp_modules=True,  # False if not finetuning MLP layers\\n            r=training_request.lora_rank,  # The larger, the higher the accuracy, but might overfit\\n            lora_alpha=training_request.lora_alpha,  # Recommended alpha == r at least\\n            lora_dropout=training_request.lora_dropout,\\n            bias=\"none\",\\n            random_state=3407,\\n            use_rslora=False,  # We support rank stabilized LoRA\\n            loftq_config=None,  # And LoftQ\\n            use_fast=True,\\n            # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\\n        )\\n    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\\n\\n    print(\"Downloading dataset\")\\n    time_start_download = time.time()\\n    response = requests.get(training_request.dataset)\\n    response.raise_for_status()  # optional: raises if request failed\\n    print(f\"Downloaded dataset in {time.time() - time_start_download} seconds\")\\n\\n    # Decode and split into lines\\n    lines = response.content.decode(\"utf-8\").splitlines()\\n\\n    # Parse and convert each JSON line\\n    time_start_convert = time.time()\\n    converted_dataset = [\\n        oai_to_unsloth(json.loads(line)) for line in lines if line.strip()\\n    ]\\n    print(f\"Converted dataset in {time.time() - time_start_convert} seconds\")\\n\\n    print(converted_dataset)\\n\\n    FastVisionModel.for_training(model)  # Enable for training!\\n\\n    trainer = SFTTrainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\\n        train_dataset=converted_dataset,\\n        args=SFTConfig(\\n            per_device_train_batch_size=training_request.batch_size,\\n            gradient_accumulation_steps=training_request.gradient_accumulation_steps,\\n            warmup_steps=training_request.warmup_steps,\\n            # max_steps=training_request.max_steps,\\n            num_train_epochs=training_request.epochs,\\n            learning_rate=training_request.learning_rate,\\n            fp16=not is_bf16_supported(),\\n            bf16=is_bf16_supported(),\\n            logging_steps=training_request.logging_steps,\\n            optim=training_request.optimizer,\\n            weight_decay=training_request.weight_decay,\\n            lr_scheduler_type=\"linear\",\\n            seed=3407,\\n            output_dir=\"outputs\",\\n            report_to=\"none\",  # For Weights and Biases\\n            # You MUST put the below items for vision finetuning:\\n            remove_unused_columns=False,\\n            dataset_text_field=\"\",\\n            dataset_kwargs={\"skip_prepare_dataset\": True},\\n            dataset_num_proc=4,\\n            max_seq_length=training_request.max_length,\\n        ),\\n    )\\n\\n    time_start_train = time.time()\\n    trainer_stats = trainer.train(resume_from_checkpoint=is_continue)\\n    print(trainer_stats)\\n    print(f\"Trained in {time.time() - time_start_train} seconds\")\\n\\n    latest_checkpoint = find_latest_checkpoint(\"outputs\")\\n    print(\"latest checkpoint\")\\n    if latest_checkpoint:\\n        print(\"Copying latest checkpoint to bucket\")\\n        bucket.copy(\\n            latest_checkpoint,\\n            adapter_uri,\\n        )\\n\\n    adapter = Adapter(\\n        name=training_request.adapter_name,\\n        uri=adapter_uri,\\n        owner=message.content.owner if message.content.owner else message.user_id,  # type: ignore\\n        base_model=training_request.model,\\n    )\\n    cache.set(cache_key, adapter.model_dump_json())\\n\\n    return TrainingResponse(\\n        loss=trainer_stats.training_loss,\\n        train_steps_per_second=trainer_stats.metrics[\"train_steps_per_second\"],\\n        train_samples_per_second=trainer_stats.metrics[\"train_samples_per_second\"],\\n        train_runtime=trainer_stats.metrics[\"train_runtime\"],\\n        adapter_name=training_request.adapter_name,\\n        adapter_uri=adapter_uri,\\n    )'}, {'key': 'FUNCTION_NAME', 'value': 'train_unsloth_sft'}, {'key': 'OUTPUT_MODEL_SOURCE', 'value': 'class TrainingResponse(BaseModel):\\n    loss: float\\n    train_steps_per_second: float\\n    train_samples_per_second: float\\n    train_runtime: float\\n    adapter_name: str\\n    adapter_uri: str'}, {'key': 'CONTENT_TYPE_SOURCE', 'value': 'class TrainingRequest(BaseModel):\\n    adapter_name: str\\n    dataset: str\\n    model: str = \"unsloth/Qwen2.5-VL-7B-Instruct\"\\n    max_length: int = 65536\\n    epochs: int = 1\\n    batch_size: int = 2\\n    gradient_accumulation_steps: int = 4\\n    learning_rate: float = 2e-4\\n    weight_decay: float = 0.01\\n    warmup_steps: int = 5\\n    logging_steps: int = 1\\n    save_steps: int = 5\\n    lora_alpha: int = 128\\n    lora_rank: int = 64\\n    lora_dropout: float = 0\\n    optimizer: str = \"adamw_8bit\"\\n    owner: Optional[str] = None'}, {'key': 'STREAM_MESSAGE_SOURCE', 'value': 'class Message(BaseModel, Generic[T]):\\n    kind: str = \"Message\"\\n    id: str\\n    content: Optional[T] = None\\n    created_at: int\\n    return_stream: Optional[str] = None\\n    user_id: Optional[str] = None\\n    orgs: Optional[Any] = None\\n    handle: Optional[str] = None\\n    adapter: Optional[str] = None\\n'}, {'key': 'PARAM_TYPE_STR', 'value': \"<class 'nebu.processors.models.Message[TrainingRequest]'>\"}, {'key': 'RETURN_TYPE_STR', 'value': \"<class '__main__.TrainingResponse'>\"}, {'key': 'IS_STREAM_MESSAGE', 'value': 'True'}, {'key': 'CONTENT_TYPE_NAME', 'value': 'TrainingRequest'}, {'key': 'MODULE_NAME', 'value': '__main__'}], 'command': 'python -m pip install dill pydantic redis nebu\\n\\n\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\\npip uninstall -y xformers\\npip install -U xformers --index-url https://download.pytorch.org/whl/cu126\\npip install unsloth trl\\n\\n\\npython -u -m nebu.processors.consumer', 'accelerators': ['1:A100_SXM'], 'restart': 'Always'}, 'min_replicas': 1, 'max_replicas': 10, 'scale': {'up': {'above_pressure': 10, 'duration': '5m'}, 'down': {'below_pressure': 2, 'duration': '10m'}, 'zero': {'duration': '10m'}}}\n",
      "Created Processor train_unsloth_sft\n",
      "[DEBUG Decorator] Processor instance 'train_unsloth_sft' created successfully.\n"
     ]
    }
   ],
   "source": [
    "class TrainingRequest(BaseModel):\n",
    "    adapter_name: str\n",
    "    dataset: str\n",
    "    model: str = \"unsloth/Qwen2.5-VL-7B-Instruct\"\n",
    "    max_length: int = 65536\n",
    "    epochs: int = 1\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 5\n",
    "    logging_steps: int = 1\n",
    "    save_steps: int = 5\n",
    "    lora_alpha: int = 128\n",
    "    lora_rank: int = 64\n",
    "    lora_dropout: float = 0\n",
    "    optimizer: str = \"adamw_8bit\"\n",
    "    owner: Optional[str] = None\n",
    "\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    loss: float\n",
    "    train_steps_per_second: float\n",
    "    train_samples_per_second: float\n",
    "    train_runtime: float\n",
    "    adapter_name: str\n",
    "    adapter_uri: str\n",
    "\n",
    "\n",
    "# TODO: add default scale\n",
    "scale = V1Scale(\n",
    "    up=V1ScaleUp(above_pressure=10, duration=\"5m\"),\n",
    "    down=V1ScaleDown(below_pressure=2, duration=\"10m\"),\n",
    "    zero=V1ScaleZero(duration=\"10m\"),\n",
    ")\n",
    "\n",
    "setup_script = \"\"\"\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip uninstall -y xformers\n",
    "pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install unsloth trl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@processor(\n",
    "    image=\"pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel\",\n",
    "    setup_script=setup_script,\n",
    "    scale=scale,\n",
    "    accelerators=[\"1:A100_SXM\"],\n",
    "    platform=\"runpod\",\n",
    ")\n",
    "def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\n",
    "    import time\n",
    "    from unsloth import FastVisionModel, is_bf16_supported\n",
    "    from unsloth.trainer import UnslothVisionDataCollator\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from nebu import (\n",
    "        Bucket,\n",
    "        ContainerConfig,\n",
    "        Cache,\n",
    "        Adapter,\n",
    "        find_latest_checkpoint,\n",
    "        is_allowed,\n",
    "        oai_to_unsloth,\n",
    "    )\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    print(\"message\", message)\n",
    "    training_request: TrainingRequest = message.content\n",
    "    if not training_request:\n",
    "        raise ValueError(\"No training request provided\")\n",
    "\n",
    "    container_config = ContainerConfig.from_env()\n",
    "    print(\"container_config\", container_config)\n",
    "\n",
    "    cache = Cache()\n",
    "    bucket = Bucket()\n",
    "\n",
    "    print(\"loading model...\")\n",
    "    adapter_uri = f\"{container_config.namespace_volume_uri}/adapters/{training_request.adapter_name}\"\n",
    "    time_start_load = time.time()\n",
    "    model = None\n",
    "\n",
    "    cache_key = f\"adapters:{training_request.adapter_name}\"\n",
    "    print(\"checking cache for adapter\", cache_key)\n",
    "    val_raw = cache.get(cache_key)\n",
    "\n",
    "    is_continue = False\n",
    "    epochs_trained = 0\n",
    "    if val_raw:\n",
    "        adapter = Adapter.model_validate_json(val_raw)\n",
    "        print(\"Found adapter: \", adapter)\n",
    "\n",
    "        epochs_trained = adapter.epochs_trained\n",
    "\n",
    "        if not is_allowed(adapter.owner, message.user_id, message.orgs):\n",
    "            raise ValueError(\"You are not allowed to train this existing adapter\")\n",
    "\n",
    "        time_start = time.time()\n",
    "        bucket.sync(adapter.uri, \"/latest\")\n",
    "        print(f\"Synced in {time.time() - time_start} seconds\")\n",
    "\n",
    "        model, tokenizer = FastVisionModel.from_pretrained(\n",
    "            \"/latest\",\n",
    "            load_in_4bit=False,\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "        )\n",
    "        is_continue = True\n",
    "    if not model:\n",
    "        print(\"Loading model from scratch\")\n",
    "        model, tokenizer = FastVisionModel.from_pretrained(\n",
    "            training_request.model,\n",
    "            load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    "        )\n",
    "\n",
    "        print(\"getting peft model...\")\n",
    "        model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers=True,  # False if not finetuning vision layers\n",
    "            finetune_language_layers=True,  # False if not finetuning language layers\n",
    "            finetune_attention_modules=True,  # False if not finetuning attention layers\n",
    "            finetune_mlp_modules=True,  # False if not finetuning MLP layers\n",
    "            r=training_request.lora_rank,  # The larger, the higher the accuracy, but might overfit\n",
    "            lora_alpha=training_request.lora_alpha,  # Recommended alpha == r at least\n",
    "            lora_dropout=training_request.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "            use_fast=True,\n",
    "            # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "        )\n",
    "    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\n",
    "\n",
    "    print(\"Downloading dataset\")\n",
    "    time_start_download = time.time()\n",
    "    response = requests.get(training_request.dataset)\n",
    "    response.raise_for_status()  # optional: raises if request failed\n",
    "    print(f\"Downloaded dataset in {time.time() - time_start_download} seconds\")\n",
    "\n",
    "    # Decode and split into lines\n",
    "    lines = response.content.decode(\"utf-8\").splitlines()\n",
    "\n",
    "    # Parse and convert each JSON line\n",
    "    time_start_convert = time.time()\n",
    "    converted_dataset = [\n",
    "        oai_to_unsloth(json.loads(line)) for line in lines if line.strip()\n",
    "    ]\n",
    "    print(f\"Converted dataset in {time.time() - time_start_convert} seconds\")\n",
    "\n",
    "    print(converted_dataset)\n",
    "\n",
    "    FastVisionModel.for_training(model)  # Enable for training!\n",
    "\n",
    "    train_epochs = epochs_trained + training_request.epochs\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
    "        train_dataset=converted_dataset,\n",
    "        args=SFTConfig(\n",
    "            per_device_train_batch_size=training_request.batch_size,\n",
    "            gradient_accumulation_steps=training_request.gradient_accumulation_steps,\n",
    "            warmup_steps=training_request.warmup_steps,\n",
    "            # max_steps=training_request.max_steps,\n",
    "            num_train_epochs=train_epochs,\n",
    "            learning_rate=training_request.learning_rate,\n",
    "            fp16=not is_bf16_supported(),\n",
    "            bf16=is_bf16_supported(),\n",
    "            logging_steps=training_request.logging_steps,\n",
    "            optim=training_request.optimizer,\n",
    "            weight_decay=training_request.weight_decay,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"outputs\",\n",
    "            report_to=\"none\",  # For Weights and Biases\n",
    "            # You MUST put the below items for vision finetuning:\n",
    "            remove_unused_columns=False,\n",
    "            dataset_text_field=\"\",\n",
    "            dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "            dataset_num_proc=4,\n",
    "            max_seq_length=training_request.max_length,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    time_start_train = time.time()\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=is_continue)\n",
    "    print(trainer_stats)\n",
    "    print(f\"Trained in {time.time() - time_start_train} seconds\")\n",
    "\n",
    "    latest_checkpoint = find_latest_checkpoint(\"outputs\")\n",
    "    print(\"latest checkpoint\")\n",
    "    if latest_checkpoint:\n",
    "        print(\"Copying latest checkpoint to bucket\")\n",
    "        bucket.copy(\n",
    "            latest_checkpoint,\n",
    "            adapter_uri,\n",
    "        )\n",
    "\n",
    "    adapter = Adapter(\n",
    "        name=training_request.adapter_name,\n",
    "        uri=adapter_uri,\n",
    "        owner=message.content.owner if message.content.owner else message.user_id,  # type: ignore\n",
    "        base_model=training_request.model,\n",
    "        epochs_trained=train_epochs,\n",
    "        last_trained=int(time.time()),\n",
    "        lora_rank=training_request.lora_rank,\n",
    "        lora_alpha=training_request.lora_alpha,\n",
    "        lora_dropout=training_request.lora_dropout,\n",
    "    )\n",
    "    cache.set(cache_key, adapter.model_dump_json())\n",
    "\n",
    "    return TrainingResponse(\n",
    "        loss=trainer_stats.training_loss,\n",
    "        train_steps_per_second=trainer_stats.metrics[\"train_steps_per_second\"],\n",
    "        train_samples_per_second=trainer_stats.metrics[\"train_samples_per_second\"],\n",
    "        train_runtime=trainer_stats.metrics[\"train_runtime\"],\n",
    "        adapter_name=training_request.adapter_name,\n",
    "        adapter_uri=adapter_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_req = TrainingRequest(\n",
    "    adapter_name=\"foo1\",\n",
    "    dataset=\"https://storage.googleapis.com/orign/testdata/nebu/clinton.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'stream_id': '1744395616208-0',\n",
       " 'message_id': '12r7LQkPqTy6h3LYcJgKWd'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unsloth_sft.send(training_req.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unsloth_sft.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nebu.processors.decorate import processor\n",
    "from nebu import Message\n",
    "from nebu.chatx.openai import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionChoice,\n",
    "    ChatCompletionResponseMessage,\n",
    "    Logprobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_script = \"\"\"\n",
    "pip install torch torchvision torchaudio qwen-vl-utils --index-url https://download.pytorch.org/whl/cu126\n",
    "pip uninstall -y xformers\n",
    "pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install tiktoken unsloth qwen-vl-utils transformers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@processor(\n",
    "    image=\"pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel\",\n",
    "    setup_script=setup_script,\n",
    "    accelerators=[\"1:A100_SXM\"],\n",
    "    platform=\"runpod\",\n",
    ")\n",
    "def infer_qwen_vl(\n",
    "    message: Message[ChatCompletionRequest],\n",
    ") -> ChatCompletionResponse:\n",
    "    import time\n",
    "\n",
    "    full_time = time.time()\n",
    "\n",
    "    import uuid\n",
    "    from unsloth import FastVisionModel\n",
    "    from qwen_vl_utils import process_vision_info\n",
    "    from nebu import (\n",
    "        Bucket,\n",
    "        ContainerConfig,\n",
    "        Adapter,\n",
    "        Cache,\n",
    "        is_allowed,\n",
    "    )\n",
    "    from transformers import AutoProcessor\n",
    "\n",
    "    base_model_id = \"unsloth/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "    if not hasattr(infer_qwen_vl, \"model\"):\n",
    "        print(\"loading model...\")\n",
    "        time_start_load = time.time()\n",
    "        base_model = FastVisionModel.from_pretrained(\n",
    "            base_model_id,\n",
    "            load_in_4bit=False,\n",
    "        )\n",
    "        print(f\"Loaded model in {time.time() - time_start_load} seconds\")\n",
    "        model_processor = AutoProcessor.from_pretrained(base_model_id)\n",
    "        FastVisionModel.for_inference(base_model)\n",
    "\n",
    "        # we can just store our model on the function\n",
    "        infer_qwen_vl.base_model = base_model  # type: ignore\n",
    "        infer_qwen_vl.model_processor = model_processor  # type: ignore\n",
    "\n",
    "    else:\n",
    "        base_model = infer_qwen_vl.base_model  # type: ignore\n",
    "        model_processor = infer_qwen_vl.model_processor  # type: ignore\n",
    "\n",
    "    if not hasattr(infer_qwen_vl, \"cache\"):\n",
    "        print(\"Creating cache\")\n",
    "        cache = Cache()\n",
    "        infer_qwen_vl.cache = cache  # type: ignore\n",
    "    else:\n",
    "        cache = infer_qwen_vl.cache  # type: ignore\n",
    "\n",
    "    if not hasattr(infer_qwen_vl, \"adapters\"):\n",
    "        print(\"Creating adapters\")\n",
    "        adapters: List[Adapter] = []\n",
    "        infer_qwen_vl.adapters = adapters  # type: ignore\n",
    "    else:\n",
    "        adapters = infer_qwen_vl.adapters  # type: ignore\n",
    "\n",
    "    print(\"message\", message)\n",
    "    training_request = message.content\n",
    "    if not training_request:\n",
    "        raise ValueError(\"No training request provided\")\n",
    "\n",
    "    print(\"content\", message.content)\n",
    "\n",
    "    container_config = ContainerConfig.from_env()\n",
    "    print(\"container_config\", container_config)\n",
    "\n",
    "    content = message.content\n",
    "    if not content:\n",
    "        raise ValueError(\"No content provided\")\n",
    "\n",
    "    adapter_hot_start = time.time()\n",
    "    val_raw = cache.get(f\"adapters:{content.model}\")\n",
    "    if val_raw:\n",
    "        print(\"val_raw\", val_raw)\n",
    "        val = Adapter.model_validate_json(val_raw)\n",
    "\n",
    "        if not is_allowed(val.owner, message.user_id, message.orgs):\n",
    "            raise ValueError(\"You are not allowed to use this adapter\")\n",
    "\n",
    "        if not val.base_model == base_model_id:\n",
    "            raise ValueError(\n",
    "                \"The base model of the adapter does not match the model you are trying to use\"\n",
    "            )\n",
    "\n",
    "        loaded = False\n",
    "        for adapter in adapters:\n",
    "            if val.name == content.model and val.created_at == adapter.created_at:\n",
    "                loaded = True\n",
    "                print(\"adapter already loaded\", content.model)\n",
    "                break\n",
    "        print(f\"Adapter hot start: {time.time() - adapter_hot_start} seconds\")\n",
    "\n",
    "        if not loaded:\n",
    "            bucket = Bucket()\n",
    "            print(\"copying adapter\", val.uri, f\"./adapters/{content.model}\")\n",
    "            time_start = time.time()\n",
    "            bucket.copy(val.uri, f\"./adapters/{content.model}\")\n",
    "            print(f\"Copied in {time.time() - time_start} seconds\")\n",
    "\n",
    "            print(\"loading adapter\", content.model)\n",
    "            base_model.load_adapter(\n",
    "                f\"./adapters/{content.model}\", adapter_name=content.model\n",
    "            )\n",
    "            infer_qwen_vl.adapters.append(content.model)  # type: ignore\n",
    "            print(\"loaded adapter\", content.model)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Adapter '{content.model}' not found\")\n",
    "\n",
    "    base_model.set_adapter(content.model)\n",
    "\n",
    "    conent_dict = content.model_dump()\n",
    "    messages = conent_dict[\"messages\"]\n",
    "\n",
    "    # Preparation for inference\n",
    "    print(\"preparing inputs\")\n",
    "    inputs_start = time.time()\n",
    "    text = model_processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = model_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    print(\"inputs\", inputs)\n",
    "    print(f\"Inputs prepared in {time.time() - inputs_start} seconds\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = base_model.generate(**inputs, max_new_tokens=content.max_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    generation_start = time.time()\n",
    "    output_text = model_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "    print(\"output_text\", output_text)\n",
    "    print(f\"Generation took {time.time() - generation_start} seconds\")\n",
    "\n",
    "    # Build the Pydantic model, referencing your enumerations and classes\n",
    "    response = ChatCompletionResponse(\n",
    "        id=str(uuid.uuid4()),\n",
    "        created=int(time.time()),\n",
    "        model=content.model,\n",
    "        object=\"chat.completion\",\n",
    "        choices=[\n",
    "            ChatCompletionChoice(\n",
    "                index=0,\n",
    "                finish_reason=\"stop\",  # or another appropriate reason\n",
    "                message=ChatCompletionResponseMessage(\n",
    "                    role=\"assistant\", content=output_text[0]\n",
    "                ),\n",
    "                # Stub logprobs; in reality, you'd fill these from your model if you have them\n",
    "                logprobs=Logprobs(content=[]),\n",
    "            )\n",
    "        ],\n",
    "        service_tier=None,\n",
    "        system_fingerprint=None,\n",
    "        usage=None,\n",
    "    )\n",
    "    print(f\"Total time: {time.time() - full_time} seconds\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
